# Journal — {{2025-09-27}} — {{Building Data Pipeline - Day 3 }}

## 1) What I learned (bullets, not prose)
- I now have a much clearer understanding of the full flow of building a data pipeline
- Learned the process of moving data from the source (PostgreSQL) through different layers
- Understood the structure and purpose of each layer:
  - **Raw** — stores the data as-is from the source
  - **Clean** — where data cleaning and transformations are applied
  - **Mart** — prepared for analytics, structured using a **star schema**
- Saw how **Metabase** is used to answer business questions using data from the mart layer

## 2) New vocabulary (define in your own words)
- **Raw layer** — The stage where data is stored exactly as it was from the source, without any changes or processing  
- **Clean layer** — A version of the data that has been cleaned and transformed to make it easier and more accurate to analyze  
- **Mart layer** — The analytics-ready layer where data is organized (often using a star schema) for business users to query  
- **Star schema** — A type of database design where a central fact table connects to multiple dimension tables; helps simplify querying  

## 3) Data Engineering mindset applied (what principles did I use?)
- Followed the **"layered architecture"** principle by separating raw, clean, and mart layers for better data organization and traceability

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Placed dimension tables in the **clean layer** instead of the **mart layer**
  - *Why:* We assumed the dimension tables were "clean enough" and didn’t need further transformation
  - *Trade-off:* Metabase couldn’t access those tables easily, which made it harder to answer business questions
  - *Lesson:* Important dimension data should also be included in the mart layer if it's needed for reporting

  - Chose a **star schema** in the mart layer to support simplified reporting
  - *Why:* Easier for non-technical users to build dashboards
  - *Alternative:* snowflake schema or using more normalized data, but those are harder to query

## 5) Open questions (things I still don’t get)
- While I understand the overall pipeline flow, I still struggle to keep up with the technical steps involved  
- I rely on others for help, especially when it comes to moving data between layers (e.g., raw → clean → mart)  
- I understand the flow but cannot yet build or execute it independently  
- I really need someone to show me, step-by-step, how they do it in practice  
- I’m confused about how data ingestion works — how does data move from PostgreSQL to the raw layer?  
- I also don’t fully grasp what changes need to be made in the code during each step (e.g., dbt models or config files)  
- I want to understand how everything connects, not just conceptually, but also through the actual code and tools  

## 6) Next actions (small, doable steps)
- [ ] Review and follow a detailed tutorial or recording showing how to move data from PostgreSQL to the raw layer  
- [ ] Practice writing or modifying ingestion code/config step-by-step with guidance  
- [ ] Explore the dbt models related to the clean and mart layers to understand their structure and transformations  
- [ ] Ask for a walkthrough or pair programming session focusing on one layer at a time  
- [ ] Take notes on each step during the walkthrough to create my own “cheat sheet” for future reference  
- [ ] Experiment with running Metabase queries using sample data to see how the mart layer supports business questions  
- [ ] Gradually try to replicate simple parts of the pipeline independently, starting with small data sets  

## 7) Artifacts & links (code, queries, dashboards)
- Group 4 Exercise Document - https://github.com/EzekielleGambong/ftw--bootcamp-grp4/blob/main/DOC-GUIDE.md
---

### Mini reflection (3–5 sentences)

Building this data pipeline helped me understand the importance of organizing data in clear layers to make analysis easier. While I grasp the overall flow, I realize I still need more hands-on practice to confidently implement each step on my own. Moving forward, I plan to focus on learning the technical details through guided tutorials and real examples. I’m excited to deepen my skills and contribute more effectively to data engineering projects.


### BONUS: What is a meme that best describes what you feel or your learning today?
Understanding the fundamentals of Data Engineering; directly using drag & drop tools without knowing how it works in the backend
